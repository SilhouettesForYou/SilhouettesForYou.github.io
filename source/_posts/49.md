---
title: Life of a triangle
toc: true
date: 2023-04-20 22:01:48
categories: 图形学
category_bar: true
tags:
- Graphics
- 翻译
- GPU
mathjax: true
---

[Life of a triangle](https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline)

Since the release of the ground breaking Fermi architecture almost 5 years have gone by, it might be time to refresh the principle graphics architecture beneath it. Fermi was the first NVIDIA GPU implementing a fully scalable graphics engine and its core architecture can be found in Kepler as well as Maxwell. The following article and especially the "compressed pipeline knowledge" image below should serve as a primer based on the various public materials, such as whitepapers or GTC tutorials about the GPU architecture. This article focuses on the graphics viewpoint on how the GPU works, although some principles such as how shader program code gets executed is the same for compute.

<font color="#BEC6A0">自从开创性的 Fermi 架构发布以来，已经过去了近五年时间，现在揭开其背后的基础图形架构了。Fermi 是 NVIDIA 首款实现完全可扩展图形引擎的 GPU，其核心架构也应用到 Kepler 和 Maxwell 的架构中。以下内容，特别是“管线缩略图”，广泛被各种 GPU 架构相关资料（如白皮书或 GTC 教程）所引用。本文从图形角度出发探讨 GPU 的工作原理，其中涉及到的着色器代码如何运行等原理和实际应用中是相同的。</font>

{% note info %}

<ul class="fa-ul" style="--fa-li-margin: -0.5em;">
  <li><span class="fa-li"><i class="fa-solid fa-link"></i></span><a href="https://www.hardwarebg.com/b4k/files/nvidia_gf100_whitepaper.pdf">Fermi Whitepaper</a></li>
  <li><span class="fa-li"><i class="fa-solid fa-link"></i></span><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf">Kepler Whitepaper</a></li>
  <li><span class="fa-li"><i class="fa-solid fa-link"></i></span><a href="https://www.techpowerup.com/gpu-specs/docs/nvidia-gtx-750-ti.pdf">Maxwell Whitepaper</a></li>
  <li><span class="fa-li"><i class="fa-solid fa-link"></i></span><a href="https://www.highperformancegraphics.org/previous/www_2010/media/Hot3D/HPG2010_Hot3D_NVIDIA.pdf">Fast Tessellated Rendering on Fermi GF100</a></li>
  <li><span class="fa-li"><i class="fa-solid fa-link"></i></span><a href="https://on-demand.gputechconf.com/gtc/2013/presentations/S3466-Programming-Guidelines-GPU-Architecture.pd">Programming Guidelines and GPU Architecture Reasons Behind Them</a></li>
</ul>

{% endnote %}

## Pipeline Architecture Image

<center>
    <img src="49/fermi-pipeline.png" alt="" />
</center>

## GPUs are Super Parallel Work Distributors

Why all this complexity? In graphics we have to deal with data amplification that creates lots of variable workloads. Each drawcall may generate a different amount of triangles. The amount of vertices after clipping is different from what our triangles were originally made of. After back-face and depth culling, not all triangles may need pixels on the screen. The screen size of a triangle can mean it requires millions of pixels or none at all.

<font color="#BEC6A0">在图形处理中，需要进行数据增强（包括但不限于裁剪、缩放、旋转、翻转、色彩变换、添加噪声等），对整个图形工作流有很大负担。每个drawcall都会会产生不同数量的三角形。裁剪后三角形顶点数量可能与原始三角形顶点数量不同。在背面剔除和深度剔除之后，并非所有三角形都需要在屏幕上显示。屏幕上的三角形需要上百万个像素组成，也可以不需要任何像素表示。这就是图形处理中复杂性的原因。我们需要处理这些变化，以确保图像能够正确、高效地渲染出来。</font>

As a consequence modern GPUs let their primitives (triangles, lines, points) follow a logical pipeline, not a physical pipeline. In the old days before G80's unified architecture (think DX9 hardware, ps3, xbox360), the pipeline was represented on the chip with the different stages and work would run through it one after another. G80 essentially reused some units for both vertex and fragment shader computations, depending on the load, but it still had a serial process for the primitives/rasterization and so on. With Fermi the pipeline became fully parallel, which means the chip implements a logical pipeline (the steps a triangle goes through) by reusing multiple engines on the chip.

<font color="#BEC6A0">简单来说，现代GPU让基本图元（如三角形、线条、点）遵循逻辑管线，而不是物理管线。在G80统一架构出现之前（比如DX9硬件、PS3、Xbox300等），GPU的处理流程在芯片上被划分为不同的阶段，并且图元会按照一定的顺序一个接着一个的处理。G80架构虽然在一定程度上实现了顶点和片元着色器计算的单元复用，但基本图元（如三角形）和栅格化等处理仍然是串行的。而到了Fermi架构，处理流程实现了全并行化，这意味着GPU通过复用芯片上的多个引擎，来并行地实现一个逻辑管线（即三角形等图元经历的各个处理步骤）。这样的设计大大提高了GPU的效率和性能。</font>

Let's say we have two triangles A and B. Parts of their work could be in different logical pipeline steps. A has already been transformed and needs to be rasterized. Some of its pixels could be running pixel-shader instructions already, while others are being rejected by depth-buffer (Z-cull), others could be already being written to framebuffer, and some may actually wait. And next to all that, we could be fetching the vertices of triangle B. So while each triangle has to go through the logical steps, lots of them could be actively processed at different steps of their lifetime. The job (get drawcall's triangles on screen) is split into many smaller tasks and even subtasks that can run in parallel. Each task is scheduled to the resources that are available, which is not limited to tasks of a certain type (vertex-shading parallel to pixel-shading).

<font color="#BEC6A0">可以这样来解释：假设我们有两个三角形A和B。它们的工作流程中可能包含不同的逻辑管线。三角形A已经完成了变换，接下来需要进行光栅化。在这个过程中，A的一些像素可能已经在执行像素着色器指令，而另一些可能因为深度缓冲区（Z-cull）而被拒绝，还有一些可能已经被写入到帧缓冲区中，还有一些可能还在等待处理。与此同时，我们可能正在获取三角形B的顶点。所以，虽然每个三角形都需要按照逻辑管线进行处理，但在它们的生命周期中，很多步骤都可以并行地进行处理。整个任务（即将drawcall后的三角形显示在屏幕上）被分割成许多更小的任务或者子任务，这些子任务可以并行运行。每个子任务都被调度到可用的资源上，包括某些特定的任务（如顶点着色器与像素着色器也可以并行处理）。这样的设计可以大大提高图形渲染的效率和性能。</font>

Think of a river that fans out. Parallel pipeline streams, that are independent of each other, everyone on their own time line, some may branch more than others. If we would color-code the units of a GPU based on the triangle, or drawcall it's currently working on, it would be multi-color blinkenlights :)

<font color="#BEC6A0">可以将这个架构想象成河流的支流，形成的多条并行的支流，它们彼此独立，各自按照自己的时间线流动，有些模块可能会生成更多的支流。如果我们根据GPU中当前正在处理的三角形或drawcall来给GPU中的单元模块染色，那么它们就会像各种颜色的信号灯一样闪烁:)</font>

## GPU Architecture

<center>
    <img src="49/pipeline-maxwell-gpu.png" alt="" />
</center>

Since Fermi NVIDIA has a similar principle architecture. There is a Giga Thread Engine which manages all the work that's going on. The GPU is partitioned into multiple GPCs (Graphics Processing Cluster), each has multiple SMs (Streaming Multiprocessor) and one Raster Engine. There is lots of interconnects in this process, most notably a Crossbar that allows work migration across GPCs or other functional units like ROP (render output unit) subsystems.

<font color="#BEC6A0">英伟达（NVIDIA）的Fermi架构遵循着一种类似的原理。它拥有一个千兆线程引擎（Giga Thread Engine），该引擎负责管理所有的工作任务。GPU被划分为多个图形处理集群（GPC），每个GPC都包含多个流多处理器（SM）和一个光栅引擎（Raster Engine）。在这个过程中，存在大量的互连，其中最重要的是一个交叉开关（Crossbar），它允许工作任务在GPC之间或像ROP（渲染输出单元）子系统等其他功能单元之间迁移。这种设计提高了GPU在处理复杂图形和数据密集型任务时的效率和灵活性。</font>

The work that a programmer thinks of (shader program execution) is done on the SMs. It contains many Cores which do the math operations for the threads. One thread could be a vertex-, or pixel-shader invocation for example. Those cores and other units are driven by Warp Schedulers, which manage a group of 32 threads as warp and hand over the instructions to be performed to Dispatch Units. The code logic is handled by the scheduler and not inside a core itself, which just sees something like "sum register 4234 with register 4235 and store in 4230" from the dispatcher. A core itself is rather dumb, compared to a CPU where a core is pretty smart. The GPU puts the smartness into higher levels, it conducts the work of an entire ensemble (or multiple if you will).

<font color="#BEC6A0"></font>

How many of these units are actually on the GPU (how many SMs per GPC, how many GPCs..) depends on the chip configuration itself. As you can see above GM204 has 4 GPCs with each 4 SMs, but Tegra X1 for example has 1 GPC and 2 SMs, both with Maxwell design. The SM design itself (number of cores, instruction units, schedulers...) has also changed over time from generation to generation (see first image) and helped making the chips so efficient they can be scaled from high-end desktop to notebook to mobile.

<font color="#BEC6A0"></font>

## The Logical Pipeline

For the sake of simplicity several details are omitted. We assume the drawcall references some index- and vertexbuffer that is already filled with data and lives in the DRAM of the GPU and uses only vertex- and pixelshader (GL: fragmentshader).

<font color="#BEC6A0"></font>

<center>
    <img src="49/fermi-pipeline-begin.png" alt="" />
</center>

1. The program makes a drawcall in the graphics api (DX or GL). This reaches the driver at some point which does a bit of validation to check if things are "legal" and inserts the command in a GPU-readable encoding inside a pushbuffer. A lot of bottlenecks can happen here on the CPU side of things, which is why it is important programmers use apis well, and techniques that leverage the power of today's GPUs.

    <font color="#BEC6A0"></font>

2. After a while or explicit "flush" calls, the driver has buffered up enough work in a pushbuffer and sends it to be processed by the GPU (with some involvement of the OS). The Host Interface of the GPU picks up the commands which are processed via the Front End.

    <font color="#BEC6A0"></font>

3. We start our work distribution in the Primitive Distributor by processing the indices in the indexbuffer and generating triangle work batches that we send out to multiple GPCs.

    <font color="#BEC6A0"></font>

    <center>
        <img src="49/fermi-pipeline-sm.png" alt="" />
    </center>

4. Within a GPC, the Poly Morph Engine of one of the SMs takes care of fetching the vertex data from the triangle indices (Vertex Fetch).

    <font color="#BEC6A0"></font>

5. After the data has been fetched, warps of 32 threads are scheduled inside the SM and will be working on the vertices.

    <font color="#BEC6A0"></font>

6. The SM's warp scheduler issues the instructions for the entire warp in-order. The threads run each instruction in lock-step and can be masked out individually if they should not actively execute it. There can be multiple reasons for requiring such masking. For example when the current instruction is part of the "if (true)" branch and the thread specific data evaluated "false", or when a loop's termination criteria was reached in one thread but not another. Therefore having lots of branch divergence in a shader can increase the time spent for all threads in the warp significantly. Threads cannot advance individually, only as a warp! Warps, however, are independent of each other.

    <font color="#BEC6A0"></font>

7. The warp's instruction may be completed at once or may take several dispatch turns. For example the SM typically has less units for load/store than doing basic math operations.

    <font color="#BEC6A0"></font>

8. As some instructions take longer to complete than others, especially memory loads, the warp scheduler may simply switch to another warp that is not waiting for memory. This is the key concept how GPUs overcome latency of memory reads, they simply switch out groups of active threads. To make this switching very fast, all threads managed by the scheduler have their own registers in the register-file. The more registers a shader program needs, the less threads/warps have space. The less warps we can switch between, the less useful work we can do while waiting for instructions to complete (foremost memory fetches).

    <font color="#BEC6A0"></font>

    <center>
        <img src="49/fermi-pipeline-memory-flow.png" alt="" />
    </center>

9. Once the warp has completed all instructions of the vertex-shader, it's results are being processed by Viewport Transform. The triangle gets clipped by the clipspace volume and is ready for rasterization. We use L1 and L2 Caches for all this cross-task communication data.

    <font color="#BEC6A0"></font>

    <center>
        <img src="49/fermi-pipeline-raster.png" alt="" />
    </center>

10. Now it gets exciting, our triangle is about to be chopped up and potentially leaving the GPC it currently lives on. The bounding box of the triangle is used to decide which raster engines need to work on it, as each engine covers multiple tiles of the screen. It sends out the triangle to one or multiple GPCs via the Work Distribution Crossbar. We effectively split our triangle into lots of smaller jobs now.

    <font color="#BEC6A0"></font>

    <center>
        <img src="49/fermi-pipeline-mid.png" alt="" />
    </center>

11. Attribute Setup at the target SM will ensure that the interpolants (for example the outputs we generated in a vertex-shader) are in a pixel shader friendly format.

    <font color="#BEC6A0"></font>

12. The Raster Engine of a GPC works on the triangle it received and generates the pixel information for those sections that it is responsible for (also handles back-face culling and Z-cull).

    <font color="#BEC6A0"></font>

13. Again we batch up 32 pixel threads, or better say 8 times 2x2 pixel quads, which is the smallest unit we will always work with in pixel shaders. This 2x2 quad allows us to calculate derivatives for things like texture mip map filtering (big change in texture coordinates within quad causes higher mip). Those threads within the 2x2 quad whose sample locations are not actually covering the triangle, are masked out (gl_HelperInvocation). One of the local SM's warp scheduler will manage the pixel-shading task.

    <font color="#BEC6A0"></font>

14. The same warp scheduler instruction game, that we had in the vertex-shader logical stage, is now performed on the pixel-shader threads. The lock-step processing is particularly handy because we can access the values within a pixel quad almost for free, as all threads are guaranteed to have their data computed up to the same instruction point (NV_shader_thread_group).

    <font color="#BEC6A0"></font>

    <center>
        <img src="49/fermi-pipeline-end.png" alt="" />
    </center>

15. Are we there yet? Almost, our pixel-shader has completed the calculation of the colors to be written to the rendertargets and we also have a depth value. At this point we have to take the original api ordering of triangles into account before we hand that data over to one of the ROP (render output unit) subsystems, which in itself has multiple ROP units. Here depth-testing, blending with the framebuffer and so on is performed. These operations need to happen atomically (one color/depth set at a time) to ensure we don't have one triangle's color and another triangle's depth value when both cover the same pixel. NVIDIA typically applies memory compression, to reduce memory bandwidth requirements, which increases "effective" bandwidth (see GTX 980 pdf).

    <font color="#BEC6A0"></font>
