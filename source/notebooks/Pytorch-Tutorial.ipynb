{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e219f4f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, root_dir, label_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.path = os.path.join(self.root_dir, self.label_dir)\n",
    "        self.img_path = os.listdir(self.path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.img_path[idx]\n",
    "        image_path = os.path.join(self.root_dir, self.label_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "        label = self.label_dir\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68471547",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70107924",
   "metadata": {},
   "source": [
    "#### Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3114fa8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"logs\")\n",
    "for i in range(100):\n",
    "    writer.add_scalar(\"y = x\", i, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26608074",
   "metadata": {},
   "source": [
    "#### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5011741",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "image_path = \"test/train/ants_image/1099452230_d1949d3250.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image_array = np.array(image)\n",
    "# print(type(image_array))\n",
    "# print(image_array.shape)\n",
    "\n",
    "writer.add_image(\"test\", image_array, 2, dataformats='HWC')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adaeae2",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "* `totensor`\n",
    "* `resize`\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0c8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 335, 500]) <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x335 at 0x230923CEF28>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "image_path = \"test/train/ants_image/1099452230_d1949d3250.jpg\"\n",
    "image = Image.open(image_path)\n",
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_image = tensor_trans(image)\n",
    "print(tensor_image.shape, image)\n",
    "image = cv2.imread(image_path)\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0294d42d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "image = Image.open(\"test/train/ants_image/1099452230_d1949d3250.jpg\")\n",
    "\n",
    "# ToTensor\n",
    "trans_to_tensor = transforms.ToTensor()\n",
    "image_tensor = trans_to_tensor(image)\n",
    "writer.add_image(\"ToTensor\", image_tensor)\n",
    "\n",
    "# Noralize\n",
    "trans_normalize = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "image_norm = trans_normalize(image_tensor)\n",
    "writer.add_image(\"Normalize\", image_norm)\n",
    "\n",
    "# Resize\n",
    "trans_resize = transforms.Resize((512, 512))\n",
    "image_resize = trans_resize(image)\n",
    "image_resize = trans_to_tensor(image_resize)\n",
    "writer.add_image(\"Resize\", image_resize, 0)\n",
    "\n",
    "# Compose 串联多个图像变换操作，因为最后要放到tensorboard里面，所以最后要用ToTensor\n",
    "trans_resize_2 = transforms.Resize(512)\n",
    "trans_compose = transforms.Compose([trans_resize_2, trans_to_tensor])\n",
    "image_resize_2 = trans_compose(image)\n",
    "writer.add_image(\"Resize\", image_resize_2, 1)\n",
    "\n",
    "# RandomCrop\n",
    "trans_random_crop = transforms.RandomCrop(300)\n",
    "trans_compose = transforms.Compose([trans_random_crop, trans_to_tensor])\n",
    "for i in range(10):\n",
    "    image_crop = trans_compose(image)\n",
    "    writer.add_image(\"RandomCrop\", image_crop, i)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530bf7c0",
   "metadata": {},
   "source": [
    "### DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5782920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "data_set_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root = \"./datasets\",\n",
    "    train = True,\n",
    "    transform = data_set_transforms,\n",
    "    download = True\n",
    ")\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root = \"./datasets\", \n",
    "    train = False, \n",
    "    transform = data_set_transforms, \n",
    "    download = True\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "for i in range(10):\n",
    "    img, target = test_set[i]\n",
    "    writer.add_image(\"test set\", img, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90a184",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "* `batch_size` - how many smaples per batch to load (default: 1)\n",
    "* `shuffle` - set to `True` to have the data reshufflled at every epoch(default: `False`)\n",
    "* `num_workers` - how many subprocess to use for data loading 0 means that the data will be loaded in the main process (default: 0)\n",
    "* `drop_last` - set to `True` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If `false` and size of dataset is not divisible by the batch size, then the last batch will be smaller (default: `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1799106",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    \"./datasets\",\n",
    "    train = False,\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_data,\n",
    "    batch_size = 64,\n",
    "    shuffle = True,\n",
    "    num_workers = 1,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "for epoch in range(2):\n",
    "    step = 0\n",
    "    for data in test_loader:\n",
    "        imgs, targets = data\n",
    "        writer.add_images(\"epoch {}\".format(epoch), imgs, step)\n",
    "        step = step + 1\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053da99",
   "metadata": {},
   "source": [
    "### torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "049e66ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NNTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input + 1\n",
    "        return output\n",
    "    \n",
    "test = NNTest()\n",
    "x = torch.tensor(1.0)\n",
    "print(test(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5fff79",
   "metadata": {},
   "source": [
    "#### CONV2D\n",
    "```python\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "    stride = 1,\n",
    "    padding = 0,\n",
    "    dilation = 1,\n",
    "    groups = 1,\n",
    "    bias = True,\n",
    "    padding_mode = 'zeros',\n",
    "    device = None,\n",
    "    dtype = None\n",
    ")\n",
    "```\n",
    "Applies a 2D convolution over an input signal composed of several input planes.\n",
    "In the simplest case, the output value of the layer with input size $(N, C_{in}, H, W)$ and output $(N, C_{out}, H_{out}, W_{out})$ can be precisely described as:\n",
    "$$out(N_{i}, C_{out_j}) = bias(C_{out_j} + \\sum_{k = 0}^{C_{in} - 1}{weight(C_{out_j}, k)} \\ast input(N_{i}, k))$$\n",
    "where $\\ast$ is the valid 2D cross-correlation operator, $N$ is a batch size, $C$ denotes a number of channels, $H$ is a height of input planes in pixels, and $W$ is width in pixels.\n",
    "\n",
    "* `stride` controls the stride for the cross-correlation, a single number or a tuple\n",
    "* `padding` controls the amount of padding applied to the input. It can be either a string {'valid', 'same'} or an int / a tuple of ints giving the amount of implicit padding applied on both sides.\n",
    "* `dilation`(扩张，扩大) controls the spacing between the kernel points; also known as the a trous algorithm (多孔算法).[visualization](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n",
    "* `groups` controls the connections between inputs and outputs. `in_channels` and `out_channels` must be both be divisible by `groups`. For example,\n",
    "    * At `groups = 1`, all inputs are convolved to all outputs\n",
    "    * At `groups = 2`, the operation becomes equivalent(等同的，等效的) to having two conv layers side by side(并排；并肩；一起), each seeing half the input channels and producing half the output channels, and both subsequently concatenated\n",
    "    * At `groups = in_channels`, each input channel is convolved with its own set of filters(of size $\\frac{out\\_channels}{in\\_channels}$)\n",
    "    \n",
    "##### Parameters\n",
    "* `in_channels` - Number of channels in the input image\n",
    "* `out_channels` - Number of channels produced by the convolution\n",
    "* `kernel_size` - Size of the convolving kernel\n",
    "* `stride` - Stride of the convolution. Default: 1\n",
    "* `padding` - Padding added to all four siders of the input. Default: 0\n",
    "* `padding_mode` - `zeros`, `reflect`, `replicate` or `circular`. Default: `zeros`\n",
    "* `dilation` - Spacing between kernel elements. Default: 1\n",
    "* `groups` - Number of blocked connections from input channels to output channels. Default: 1\n",
    "* `bias` - If `True`, adds a learnable bias to the output. Default: `False`\n",
    "\n",
    "##### Shape\n",
    "* Input: $(N, C_{in}, H_{in}, W_{in})$ or $(C_{in}, H_{in}, W_{in})$\n",
    "* Output: $(N, C_{out}, H_{out}, W_{out})$ or $(C_{out}, H_{out}, W_{out})$, where\n",
    "$$H_{out} = \\Big\\lfloor \\frac{H_{in} + 2 \\times padding[0] - dilation[0] \\times (kernel\\_size[0] - 1) - 1}{stride[0]} + 1 \\Big\\rfloor$$\n",
    "$$W_{out} = \\Big\\lfloor \\frac{H_{in} + 2 \\times padding[1] - dilation[1] \\times (kernel\\_size[1] - 1) - 1}{stride[1]} + 1 \\Big\\rfloor$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8e26832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./datasets/\",\n",
    "    train = False,\n",
    "    transform = torchvision.transforms.ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "class Conv2DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3,\n",
    "            out_channels = 6,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 0\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "    \n",
    "test = Conv2DNN()\n",
    "step = 0\n",
    "writer = SummaryWriter(\"logs\")\n",
    "for data in dataloader:\n",
    "    images, targets = data\n",
    "    output = test(images)\n",
    "    writer.add_images(\"input_conv2d\", images, step)\n",
    "    t = (-1, images.shape[1]) + tuple(output.shape[2:])\n",
    "    output = torch.reshape(output, t)\n",
    "    writer.add_images(\"output_conv2d\", output, step)\n",
    "    step = step + 1\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec726f1",
   "metadata": {},
   "source": [
    "#### Pooling Layers\n",
    "* `kernel_size` - the size of the window to take a max over\n",
    "* `stride` - the stride of the window. Default value is `kernel_size`\n",
    "* `padding` - Implicit(暗示的；盲从的；含蓄的) negative infinity padding to be added on both sides\n",
    "* `dilation` - a parameter that controls the stride of elements in the window\n",
    "* `return_indices` - if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpooled2d` later\n",
    "* `ceil_model` - when `True`, will use ceil instead of floor to compute the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604eaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MaxPool2d\n",
    "\n",
    "input = torch.tensor([\n",
    "    [1, 2, 0, 3, 1],\n",
    "    [0, 1, 2, 3, 1],\n",
    "    [1, 2, 1, 0, 0],\n",
    "    [5, 2, 3, 1, 1],\n",
    "    [2, 1, 0, 1, 1]\n",
    "], dtype=torch.float32)\n",
    "input = torch.reshape(input, (-1, 1, 5, 5))\n",
    "\n",
    "class PoolingNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingNN, self).__init__()\n",
    "        self.max_pool1 = MaxPool2d(kernel_size=3, ceil_mode=False)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.max_pool1(input)\n",
    "        return output\n",
    "    \n",
    "test = PoolingNN()\n",
    "output = test(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1229fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./datasets/\",\n",
    "    train = False,\n",
    "    transform = torchvision.transforms.ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "class PoolingNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingNN, self).__init__()\n",
    "        self.max_pool1 = MaxPool2d(kernel_size=3, ceil_mode=False)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.max_pool1(input)\n",
    "        return output\n",
    "    \n",
    "test = PoolingNN()\n",
    "step = 0\n",
    "writer = SummaryWriter(\"logs\")\n",
    "for data in dataloader:\n",
    "    images, targets = data\n",
    "    output = test(images)\n",
    "    writer.add_images(\"input_max_pooling\", images, step)\n",
    "    writer.add_images(\"output_max_pooling\", output, step)\n",
    "    step = step + 1\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3c739",
   "metadata": {},
   "source": [
    "#### Non-linear Activations\n",
    "* RELU `torch.nn.ReLU(inplace=False)`\n",
    "    * `inplace` 是否对原来的数据进行重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f87f5c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set x's range\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "y1 = 1 / (1 + math.e ** (-x))  # sigmoid\n",
    "y2 = (math.e ** (x) - math.e ** (-x)) / (math.e ** (x) + math.e ** (-x))  # tanh\n",
    "y3 = np.where(x < 0, 0, x)  # relu\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-1, 1.2)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "\n",
    "# Draw pic\n",
    "plt.plot(x, y1, label='Sigmoid', linestyle=\"-\", color=\"blue\")\n",
    "plt.plot(x, y2, label='Tanh', linestyle=\"-\", color=\"cyan\")\n",
    "plt.plot(x, y3, label='ReLU', linestyle=\"-\", color=\"green\")\n",
    "# Title\n",
    "plt.legend(['Sigmoid', 'Tanh', 'Relu'])\n",
    "plt.legend(loc='upper left')  # 将图例放在左上角\n",
    "\n",
    "# show it!!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e30d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./datasets/\",\n",
    "    train = False,\n",
    "    transform = torchvision.transforms.ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "input = torch.tensor([\n",
    "    [1, -0.5],\n",
    "    [-1, 3]\n",
    "])\n",
    "input = torch.reshape(input, (-1, 1, 2, 2))\n",
    "class ReLUNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNN, self).__init__()\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.relu1(input)\n",
    "        return output\n",
    "    \n",
    "class SigmodNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmodNN, self).__init__()\n",
    "        self.relu1 = Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.relu1(input)\n",
    "        return output\n",
    "    \n",
    "test = SigmodNN()\n",
    "# output = test(input)\n",
    "step = 0\n",
    "writer = SummaryWriter(\"logs\")\n",
    "for data in dataloader:\n",
    "    images, targets = data\n",
    "    output = test(images)\n",
    "    writer.add_images(\"input_sigmoid\", images, step)\n",
    "    writer.add_images(\"output_sigmoid\", output, step)\n",
    "    step = step + 1\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267c03c",
   "metadata": {},
   "source": [
    "#### LINEAR\n",
    "`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`\n",
    "\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "##### Parameters\n",
    "* `in_features` - size of each input simple\n",
    "* `out_features` - size of each output sample\n",
    "* `bias` - If set to `False`, the layer will not learn an additive bias. Default: `True`\n",
    "\n",
    "##### Variables\n",
    "* `weight` - the learnable weights of the module of shape $(out\\_features, in\\_features)$. The values are initialized from $u(-\\sqrt{k}, \\sqrt{k})$, where $k = \\frac{1}{in\\_features}$\n",
    "* `bias` - the learnable bias of the module of shape $(out\\_features)$. If `bias` is `True`, the values are initialized from $u(-\\sqrt{k}, \\sqrt{k})$, where $k = \\frac{1}{in\\_features}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbddec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\"./datasets/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "class LinearTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearTest, self).__init__()\n",
    "        self.linear1 = nn.Linear(196608, 10)\n",
    "        \n",
    "    def foward(self, input):\n",
    "        output = self.linear1(input)\n",
    "        \n",
    "for data in dataloader:\n",
    "    images, targets = data\n",
    "    output = torch.flatten(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8f52b",
   "metadata": {},
   "source": [
    "### SEQUENTIAL\n",
    "![](https://www.researchgate.net/profile/Yiren-Zhou-6/publication/312170477/figure/fig2/AS:448817725218817@1484017892180/Structure-of-CIFAR10-quick-model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12cc7396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class SequentialTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequentialTest, self).__init__()\n",
    "#         self.conv1 = Conv2d(32, 32, 5, padding=2)\n",
    "#         self.maxPool1 = MaxPool2d(2)\n",
    "#         self.conv2 = Conv2d(32, 32, 5, padding=2)\n",
    "#         self.maxPool2 = MaxPool2d(2)\n",
    "#         self.conv3 = Conv2d(32, 64, 5, padding=2)\n",
    "#         self.maxPool3 = MaxPool2d(2)\n",
    "#         self.flatten = Flatten()\n",
    "#         self.linear1 = Linear(1024, 64)\n",
    "#         self.linear2 = Linear(64, 10)\n",
    "        \n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.maxPool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.maxPool2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.maxPool3(x)\n",
    "#         x = self.flatten()\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.linear2(x)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        \n",
    "test = SequentialTest()\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "output = test(input)\n",
    "print(output.shape)\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_graph(test, input)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90e812",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "#### L1Loss\n",
    "Creates a criterion that measures the **mean absolute error (MAE)** between each element in the input $x$ and target $y$. The unreduced (i.e. with `reduction` set to `none`) loss can be described as:\n",
    "$$\\mathscr{l}(x, y) = L = \\{l_i, \\cdots , l_N\\}^T, l_n = |x_n - y_n|$$\n",
    "Where $N$ is the batch size. If `reduction` is not `none` (default `mean`), then:\n",
    "$$\n",
    "\\mathscr{l}(x, y) = \n",
    "\\begin{cases}\n",
    "mean(L), \\ if \\ reduction = `mean` \\\\\n",
    "sum(L), \\ if \\ rduction = `sum`\n",
    "\\end{cases}\n",
    "$$\n",
    "$x$ and $y$ are tensors of arbitrary (任意的) shapes with a total of $n$ elements each. \n",
    "\n",
    "The sum operation still operates over all the elements, and divides by $n$. The division by $n$ can be avoided if one sets `reduction` = `sum`.\n",
    "\n",
    "##### Parameters\n",
    "* `size_average` - Deprecated. By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when `reduce` is `False`. Default: `True`\n",
    "* `reduce` - Deprecated.\n",
    "* `reduction` - Specifies the reduction to apply to the output: `none` | `mean` | `sum`.\n",
    "    * `none`: no reduction will be applied\n",
    "    * `mean`: the sum of the output will be divided by the number of elements in the output\n",
    "    * `sum`: the output will be summed\n",
    "    \n",
    "#### MSELoss\n",
    "Creates a criterion that measures the **mean squared error (squared L2 norm)** between each element in the input $x$ and target $y$. The unreduced (i.e. with `reduction` set to `none`) loss can be described as:\n",
    "$$\\mathscr{l}(x, y) = L = \\{l_i, \\cdots , l_N\\}^T, l_n = (x_n - y_n)^2$$\n",
    "Where $N$ is the batch size. If `reduction` is not `none` (default `mean`), then:\n",
    "$$\n",
    "\\mathscr{l}(x, y) = \n",
    "\\begin{cases}\n",
    "mean(L), \\ if \\ reduction = `mean` \\\\\n",
    "sum(L), \\ if \\ rduction = `sum`\n",
    "\\end{cases}\n",
    "$$\n",
    "$x$ and $y$ are tensors of arbitrary (任意的) shapes with a total of $n$ elements each. \n",
    "\n",
    "#### CrossEntropyLoss\n",
    "This criterion computes the cross entropy loss between input logits and target. It is useful when training a classification problem with $C$ classes. If provided, the optional argument `weight` should be be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.\n",
    "\n",
    "The `input` is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general). `input` has to be a Tensor of size($C$) for unbatched input, (minibatch, $C$) or (minibatch, $C, d_1, d_2, \\cdots, d_K$) with $K \\geq 1$ for the K-dimensional case. The last being useful for higher dimension inputs, such as computing cross entropy loss per-pixel for 2D images.\n",
    "\n",
    "The target that this criterion expects should contain either:\n",
    "* Class indices in the range $[0, C)$ where $C$ is number of classes; if `ignore_index` is specified, this loss also accepts this class index (this index may not necessarily in the class range). The unreduced (i.e. with set to) loss for this case can be described as : `reduction` `none`\n",
    "$$\n",
    "\\mathscr{l}(x, y) = L = \\{l_1, \\cdots, l_N\\}^T, l_n = -\\mathscr{w}_{y_n}log\\frac{exp(x_{n, y_n})}{\\sum_{c=1}^{C}exp(x_n, c)} \\cdot 1 \\{y_n \\neq ignore\\_index\\}\n",
    "$$\n",
    "where $x$ is the input, $y$ is the target, $\\mathscr{w}$ is the weight, $C$ is the number of classes, and $N$ spans the minibatch dimension as well as $d_1, \\cdots, d_k$ for K-dimensional case. If is not (default), then `reduction` `none` `mean`\n",
    "$$\n",
    "\\mathscr{l}(x, y) = \n",
    "\\begin{cases}\n",
    "\\sum_{n = 1}^N{\\frac{1}{\\sum_{n = 1}^N{\\mathscr{w} \\cdot 1 \\{y_n \\neq ignore\\_index\\}}}}l_n & if \\ reduction = `mean`;\\\\\n",
    "\\sum_{n = 1}^N{l_n} & if \\ reduction = `sum`.\n",
    "\\end{cases}\n",
    "$$\n",
    "Note that this case is equivalent to applying `LogSoftmax` on an input, followed by `NLLLoss`.\n",
    "* Probabilities for each class; useful when labels beyond a single class per minibatch item are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with set to) loss for this case can be described as: `reduction` `none`\n",
    "$$\n",
    "\\mathscr{l}(x, y) = L = \\{l_1, \\cdots, l_N\\}^T, l_n = -\\sum_{c = 1}^C \\mathscr{w}_{c}log\\frac{exp(x_{n, c})}{\\sum_{c=1}^{C}exp(x_{n, i})} y_{n, c}\n",
    "$$\n",
    "where $x$ is the input, $y$ is the target, $\\mathscr{w}$ is the weight, $C$ is the number of classes, and $N$ spans the minibatch dimension as well as $d_1, \\cdots, d_k$ for the K-dimensional case. If is not (default), then `reduction` `none` `mean`\n",
    "$$\n",
    "\\mathscr{l}(x, y) = \n",
    "\\begin{cases}\n",
    "\\frac{\\sum_{n = 1}^N l_n}{N}, & if \\ reduction = `mean`; \\\\\n",
    "\\sum_{n = 1}^N{l_n}, & if \\ reduction = `sum`.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "##### Parameters\n",
    "* `weight` - a manual rescaling weight given to each class. If given, has to be a Tensor of size $C$ and floating point dtype\n",
    "* `ignore_index` - Specifies a target value that is ignored and does not contribute to the input gradient. When is ,the loss is averaged over non-ignored targets. Note that is only applicable when the target contains class indices. `size_average` `True` `ignore_index`\n",
    "* `reduction` - Specifies the reduction to apply to the output:\n",
    "    * `none`: no reduction will be applied\n",
    "    * `mean`: the weighted mean of the output is taken\n",
    "    * `sum`: the output will be summed\n",
    "* `label_smoothing` - A float in $[0.0, 1.0]$. Specifies the amount of smoothing when computing the loss, where $0.0$ mans no smoothing. The targets become a mixture of the original ground truth and a uniform distribution as described in [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567). Default: $0.0$\n",
    "\n",
    "##### Shape\n",
    "* Input: Shape$(C)$, $(N, C)$ or $(N, C, d_1, d_2, \\cdots, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss.\n",
    "* Target: If containing class indices, shape$()$, $N$ or $(N, d_1, d_2, \\cdots, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss where each value should be between $[0, C)$, If containing class probabilities, same shape as the input and each value should be between $[0, 1]$.\n",
    "* Output: If reduction is `none`, shape $()$, $(N)$ or $(N, d_1, d_2, \\cdots, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n",
    "\n",
    "where:\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= number \\ of \\ classes \\\\\n",
    "N &= batch \\ size\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a0a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0977,  0.4765, -0.8041,  0.0542, -0.2359],\n",
      "        [-0.3361, -1.1238,  1.0310,  0.3329, -1.1886],\n",
      "        [ 2.9726, -0.1864,  0.6002,  1.4690,  0.3177]], requires_grad=True) tensor([4, 2, 1]) tensor(2.0146, grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.7895, -0.6270, -1.3918,  0.0207, -0.1117],\n",
      "        [-0.0178,  0.1340, -1.0316,  0.0742, -0.9761],\n",
      "        [-0.0408, -0.4790,  0.9696,  0.5091, -0.0875]], requires_grad=True) tensor([[0.3869, 0.1456, 0.1144, 0.2700, 0.0830],\n",
      "        [0.0730, 0.1722, 0.3235, 0.3899, 0.0413],\n",
      "        [0.1855, 0.6401, 0.0980, 0.0262, 0.0502]]) tensor(1.8426, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input,target)\n",
    "output.backward()\n",
    "print(input, target, output)\n",
    "\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(input, target, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58630751",
   "metadata": {},
   "source": [
    "> `loss.backward()`：根据当前的损失值（通常是一个标量）计算模型参数的梯度。它会从损失开始，然后通过模型的各个层反向传播梯度，最终计算出每个参数相对于损失的梯度。这些梯度将被用于后续的参数更新，以减小损失；将损失$\\mathscr{l}$向输入侧进行反向传播，同于对于需要进行梯度计算的所有变量$x$ `(require_grad=True)`，计算梯度$\\frac{\\mathrm{d}}{\\mathrm{d}x}\\mathscr{l}$，并将其累积到梯度$x.grad$中备用，即：$x.grad = x.grad + \\frac{\\mathrm{d}}{\\mathrm{d}x}\\mathscr{l}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f926a30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.3333)\n",
      "tensor(1.1019)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import L1Loss\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "inputs = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "targets = torch.tensor([1, 2, 5], dtype=torch.float32)\n",
    "\n",
    "inputs = torch.reshape(inputs, (1, 1, 1, 3))\n",
    "targets = torch.reshape(targets, (1, 1, 1, 3))\n",
    "\n",
    "loss = L1Loss(reduction='sum')\n",
    "res = loss(inputs, targets)\n",
    "print(res)\n",
    "lose_mse = MSELoss()\n",
    "res = lose_mse(inputs, targets)\n",
    "print(res)\n",
    "\n",
    "x = torch.tensor([0.1, 0.2, 0.3])\n",
    "x = torch.reshape(x, (1, 3))\n",
    "y = torch.tensor([1])\n",
    "loss = CrossEntropyLoss()\n",
    "res = loss(x, y)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c51e4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CrossEntropyLossTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossTest, self).__init__()\n",
    "        \n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        \n",
    "dataset = torchvision.datasets.CIFAR10(\"./datasets/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "test = CrossEntropyLossTest()\n",
    "loss = CrossEntropyLoss()\n",
    "for data in dataloader:\n",
    "    images, targets = data\n",
    "    outputs = test(images)\n",
    "    res = loss(outputs, targets)\n",
    "    res.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435398d",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "#### How to use an optimizer\n",
    "##### Constructing it\n",
    "To Construct an Optimizer you have to give it  an iterable containing the parameters (all should be `Variable`) to optimize. Then, you can specify optimizer-specific (细节；特性) options such as the learning rate, weight decay (权重衰减), etc.\n",
    "\n",
    "##### Per-parameter options\n",
    "Optimizers also support specifying per-parameter options. To do this, instead of passing a iterable of `Variable`, pass in an iterable of dict. Each of them will define a separate parameter group, and should contain a `param` key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.\n",
    "\n",
    "For example, this is very useful when one wants to specify per-layer learning rates:\n",
    "```python\n",
    "optim.SGD([\n",
    "    {'params': model.base.parameters()},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "], lr=1e-2, momentum=0.9)\n",
    "```\n",
    "\n",
    "##### Taking an optimization step\n",
    "All optimizers implement a `step()` method, that updates the parameters. It cane be used in two ways:\n",
    "* `optimizer.step()` - This is a simplified version support by most optimizers. The function can be called once the gradients are computed using e.g. `backward()`. 梯度被计算出来之后即可调用\n",
    "* `optimizer.step(closure)` - Some optimization algorithm such as Conjugate Gradient (共轭梯度法) and LBFGS (拟牛顿法) need to reevalute the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure clear the gradients, compute the loss, and return it.\n",
    "```python\n",
    "for input, target in dataset:\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "```\n",
    "\n",
    "#### Algorithms\n",
    "##### Adadelta Algorithm\n",
    "\n",
    "* * *\n",
    "\n",
    "**input:** $\\gamma$(lr), $\\theta_0$(params), $f(\\theta)$(objective), $\\rho$(decay), $\\lambda$(weight decay)\n",
    "<br style=\"line-height:0.1;\">\n",
    "**initialize**: $v_0 \\leftarrow 0$(square avg), $u_0 \\leftarrow 0$(accumulate variables)\n",
    "* * *\n",
    "`for` $t = 1$ `to ... do`<br>\n",
    "    $\\quad g_t \\leftarrow \\nabla_\\theta f_t(\\theta_{t - 1})$<br>\n",
    "    $\\quad if \\ \\lambda \\neq 0$<br>\n",
    "        $\\qquad g_t \\leftarrow g_t + \\lambda \\theta_{t - 1}$<br>\n",
    "    $\\quad v_t \\leftarrow v_{t - 1}\\rho + g_t^2(1 - \\rho)$<br>\n",
    "    $\\quad \\Delta x_t \\leftarrow \\dfrac{\\sqrt{u_{t - 1} + \\epsilon}}{\\sqrt{v_t + \\epsilon}}g_t$<br>\n",
    "    $\\quad u_t \\leftarrow u_{t - 1}\\rho + \\Delta x_t^2(1 - \\rho)$<br>\n",
    "    $\\quad \\theta_t \\leftarrow \\theta_{t - 1}\\rho - \\gamma\\Delta x_t$<br>\n",
    "* * *\n",
    "`return` $\\theta_t$\n",
    "* * *\n",
    "<br>\n",
    "\n",
    "##### Adagrad Algorithm\n",
    "* * *\n",
    "**input:** $\\gamma$(lr), $\\theta_0$(params), $f(\\theta)$(objective), $\\rho$(decay), $\\lambda$(weight decay), $\\tau$(initial accumulator value), $\\eta$(lr decay)\n",
    "<br style=\"line-height:0.1;\">\n",
    "**initialize**: $state\\_sum_0 \\leftarrow 0$\n",
    "* * *\n",
    "`for` $t = 1$ `to ... do`<br>\n",
    "    $\\quad g_t \\leftarrow \\nabla_\\theta f_t(\\theta_{t - 1})$<br>\n",
    "    $\\quad \\tilde{\\gamma} \\leftarrow \\dfrac{\\gamma}{1 + (t - 1) \\eta} $<br>\n",
    "    $\\quad if \\ \\lambda \\neq 0$<br>\n",
    "        $\\qquad g_t \\leftarrow g_t + \\lambda \\theta_{t - 1}$<br>\n",
    "    $\\quad state\\_sum_t \\leftarrow state\\_sum_{t - 1} + g_t^2$<br>\n",
    "    $\\quad \\theta_t \\leftarrow \\theta_{t - 1}\\rho - \\tilde{\\gamma}\\dfrac{g_t}{\\sqrt{state\\_sum_t} + \\epsilon}$<br>\n",
    "* * *\n",
    "`return` $\\theta_t$\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbcf9753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "tensor(359.8171, grad_fn=<AddBackward0>)\n",
      "tensor(349.5803, grad_fn=<AddBackward0>)\n",
      "tensor(326.4633, grad_fn=<AddBackward0>)\n",
      "tensor(315.0732, grad_fn=<AddBackward0>)\n",
      "tensor(307.1474, grad_fn=<AddBackward0>)\n",
      "tensor(297.9225, grad_fn=<AddBackward0>)\n",
      "tensor(288.7407, grad_fn=<AddBackward0>)\n",
      "tensor(281.0782, grad_fn=<AddBackward0>)\n",
      "tensor(273.8957, grad_fn=<AddBackward0>)\n",
      "tensor(267.2983, grad_fn=<AddBackward0>)\n",
      "tensor(261.1341, grad_fn=<AddBackward0>)\n",
      "tensor(255.4881, grad_fn=<AddBackward0>)\n",
      "tensor(250.4390, grad_fn=<AddBackward0>)\n",
      "tensor(245.8700, grad_fn=<AddBackward0>)\n",
      "tensor(241.6462, grad_fn=<AddBackward0>)\n",
      "tensor(237.7090, grad_fn=<AddBackward0>)\n",
      "tensor(234.0188, grad_fn=<AddBackward0>)\n",
      "tensor(230.5683, grad_fn=<AddBackward0>)\n",
      "tensor(227.3083, grad_fn=<AddBackward0>)\n",
      "tensor(224.1937, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class OptimizerLossTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptimizerLossTest, self).__init__()\n",
    "        \n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizerTest = OptimizerLossTest()\n",
    "optimizer = torch.optim.SGD(optimizerTest.parameters(), lr=0.01)\n",
    "dataset = torchvision.datasets.CIFAR10(\"./datasets/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.01\n",
    "    for data in dataloader:\n",
    "        images, targets = data\n",
    "        outputs = optimizerTest(images)\n",
    "        res_loss = loss(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        res_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss = running_loss + res_loss\n",
    "    print(running_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.models\n",
    "#### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "    (add_linear): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      "  (add_linear): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "# train_data = torchvision.datasets.ImageNet(\n",
    "#     \"./datasets_image_net/\",\n",
    "#     split=\"train\",\n",
    "#     download=True,\n",
    "#     transform=torchvision.transforms.ToTensor()\n",
    "# )\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "# train_data = torchvision.datasets.CIFAR10(\"./datasets/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "vgg16.add_module(\"add_linear\", nn.Linear(1000, 10))\n",
    "vgg16.classifier.add_module(\"add_linear\", nn.Linear(1000, 10))\n",
    "vgg16.classifier[6] = nn.Linear(4096, 10)\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "第1轮训练开始\n",
      "整体测试集上的loss：316.3902188539505\n",
      "整体测试集上的正确率：0.27459999918937683\n",
      "running time: 64.21319317817688\n",
      "第2轮训练开始\n",
      "整体测试集上的loss：303.25164806842804\n",
      "整体测试集上的正确率：0.3166999816894531\n",
      "running time: 64.35326194763184\n",
      "第3轮训练开始\n",
      "整体测试集上的loss：269.6542776823044\n",
      "整体测试集上的正确率：0.3766999840736389\n",
      "running time: 64.12790966033936\n",
      "第4轮训练开始\n",
      "整体测试集上的loss：265.72354912757874\n",
      "整体测试集上的正确率：0.3953999876976013\n",
      "running time: 64.27544069290161\n",
      "第5轮训练开始\n",
      "整体测试集上的loss：251.11966288089752\n",
      "整体测试集上的正确率：0.4251999855041504\n",
      "running time: 64.14677286148071\n",
      "第6轮训练开始\n",
      "整体测试集上的loss：236.13818204402924\n",
      "整体测试集上的正确率：0.45909997820854187\n",
      "running time: 63.203404903411865\n",
      "第7轮训练开始\n",
      "整体测试集上的loss：220.12329769134521\n",
      "整体测试集上的正确率：0.4943999946117401\n",
      "running time: 63.02765965461731\n",
      "第8轮训练开始\n",
      "整体测试集上的loss：206.44537925720215\n",
      "整体测试集上的正确率：0.5300999879837036\n",
      "running time: 63.66409635543823\n",
      "第9轮训练开始\n",
      "整体测试集上的loss：196.9747644662857\n",
      "整体测试集上的正确率：0.5525999665260315\n",
      "running time: 64.37123441696167\n",
      "第10轮训练开始\n",
      "整体测试集上的loss：190.3463258743286\n",
      "整体测试集上的正确率：0.5712999701499939\n",
      "running time: 63.861103773117065\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CifarModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CifarModel, self).__init__()\n",
    "        \n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "train_data = torchvision.datasets.CIFAR10(\"./datasets/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(\"./datasets/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_data, batch_size=64)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "writer = SummaryWriter(\"logs/\")\n",
    "model = CifarModel()\n",
    "model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func.to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_train_steps = 0\n",
    "total_test_steps = 0\n",
    "epoch = 10\n",
    "for i in range(epoch):\n",
    "    print(\"第{}轮训练开始\".format(i + 1))\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, targets = data\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_steps = total_train_steps + 1\n",
    "        if total_train_steps % 100 == 0:\n",
    "            # print(\"训练次数：{}，loss：{}\".format(total_train_steps, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_steps)\n",
    "        \n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    total_test_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, targets = data\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_test_accuracy = total_test_accuracy + accuracy\n",
    "    print(\"整体测试集上的loss：{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率：{}\".format(total_test_accuracy / len(test_data)))\n",
    "    end_time = time.time()\n",
    "    print(\"running time: {}\".format(end_time - start_time))\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, total_test_steps)\n",
    "    writer.add_scalar(\"test_accuracy\", total_test_accuracy / len(test_data), total_test_steps)\n",
    "    total_test_steps = total_test_steps + 1\n",
    "    torch.save(model, \"./models/test_{}\".format(i))\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CifarModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CifarModel, self).__init__()\n",
    "        \n",
    "        self.model = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "image = Image.open(\"IMG_20231124_082111.jpg\")\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32, 32), interpolation=InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "image = transform(image)\n",
    "plt.imshow(image.transpose(0, 2).numpy())\n",
    "plt.show()\n",
    "image = torch.reshape(image, (1, 3, 32, 32))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image = image.to(device)\n",
    "model = torch.load(\"./models/test_9\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "print(output.argmax(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "697px",
    "left": "27.9844px",
    "top": "120.591px",
    "width": "391px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 449.333334,
   "position": {
    "height": "471px",
    "left": "1804.99px",
    "right": "20px",
    "top": "118.99px",
    "width": "294px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
